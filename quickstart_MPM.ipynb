{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d14365-3a88-4ad6-845a-41787c646e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "from torch.nn import MSELoss\n",
    "mse_loss = MSELoss(size_average = True)\n",
    "\n",
    "from basicsr.models import create_model, load_finetuned_model\n",
    "from basicsr.utils import img2tensor as _img2tensor, tensor2img, imwrite\n",
    "from basicsr.models.image_restoration_model import ImageRestorationModel\n",
    "from basicsr.utils.options import parse\n",
    "\n",
    "from tqdm.notebook import tqdm as log_progress\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26baf994-194f-4b30-83fe-115957728109",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e297af-18a3-499a-97c6-35239c5ccb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#Loading in our finetuned weights to the NAFNet model\n",
    "finetuned_weight_path = 'finetuned_weights.pth'\n",
    "config_path = 'model_config.yml'\n",
    "\n",
    "finetuned = load_finetuned_model(finetuned_weight_path, config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f46d5c-c281-4355-b56b-c432d6fe8231",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = np.load('quickstart_samples/MPM_samples_CH4_gt.npy',allow_pickle='TRUE').item()\n",
    "noisy_dict = np.load('quickstart_samples/MPM_samples_CH4_noisy.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86652e-5f4f-4fee-a7f7-ed2504fcc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_lambdas = [6.212425231933594, 7.675350666046143, 8.296592712402344, 8.97795581817627, 8.176352500915527, 8.897795677185059, 6.953907489776611, 7.314629077911377, 8.797595024108887, 10.0, 10.0, 8.396794319152832, 7.094188213348389, 6.673346519470215, 7.575150012969971, 7.795590877532959, 9.859719276428223, 8.476954460144043, 5.8116230964660645, 8.897795677185059]\n",
    "\n",
    "ch2_lambdas =[10.0, 10.0, 7.995992183685303, 6.352705001831055, 5.1503005027771, 4.589178085327148, 4.448897838592529, 4.52905797958374, 4.629258632659912, 3.567134141921997, 3.3867735862731934, 3.647294521331787, 3.3667335510253906, 3.1863725185394287, 4.008016109466553, 4.609218597412109, 3.887775421142578, 5.751502990722656, 4.729458808898926, 4.889779090881348]\n",
    "\n",
    "ch3_lambdas = [1.302605152130127, 1.2825651168823242, 1.302605152130127, 1.2224448919296265, 1.1623246669769287, 1.0821642875671387, 1.0420842170715332, 0.9418837428092957, 0.8617234230041504, 0.8216432929039001, 0.7414829730987549, 0.7214428782463074, 0.7414829730987549, 0.7014027833938599, 0.6813627481460571, 0.6813627481460571, 0.7014027833938599, 0.6613226532936096, 0.6212424635887146, 0.6012023687362671]\n",
    "\n",
    "ch4_lambdas = [2.525049924850464, 2.7254509925842285, 2.505009889602661, 2.685370683670044, 2.9058115482330322, 3.1462924480438232, 2.845691442489624, 2.685370683670044, 2.8056113719940186, 2.765531063079834, 3.2064127922058105, 3.086172103881836, 2.9058115482330322, 2.765531063079834, 2.7454910278320312, 2.7454910278320312, 2.4649298191070557, 2.424849510192871, 2.0841684341430664, 1.9639278650283813]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebe70e-307f-4fe7-b18f-53868a05143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "test_ssim = []\n",
    "\n",
    "denoised_dict = {}\n",
    "uncertain_dict = {}\n",
    "mse_dict = {}\n",
    "ssim_dict = {}\n",
    "\n",
    "for key in range(28, 29):\n",
    "    single_gt = gt_dict[key]\n",
    "    uncertainties, multi_predictions, mses, ssim_vals = [], [], [], []\n",
    "    single_gt = [gt for gt in single_gt] \n",
    "    single_image = noisy_dict[key]\n",
    "    \n",
    "    for i, in_img in enumerate(single_image):\n",
    "        in_img = in_img.unsqueeze(0).to(device)  # (1, 5, 512, 512)\n",
    "\n",
    "        out = finetuned(in_img.float())  # (1, 5, 3, 512, 512)\n",
    "\n",
    "        # Extract prediction and uncertainty bounds\n",
    "        upper = out[0, 2, :, :].detach().cpu()\n",
    "        pred = out[0, 1, :, :].detach().cpu()\n",
    "        lower = out[0, 0, :, :].detach().cpu()\n",
    "\n",
    "        pred /= torch.max(pred)  # Normalize prediction\n",
    "        multi_predictions.append(pred)\n",
    "\n",
    "        # Compute uncertainty\n",
    "        uncertainty = ((upper - lower) * ch4_lambdas[i]).cpu().numpy()\n",
    "        uncertainties.append(uncertainty)\n",
    "\n",
    "        # Compute MSE and SSIM\n",
    "        err = mse_loss(pred.float(), single_gt[i])\n",
    "        mses.append(err)\n",
    "        ssim_vals.append(structural_similarity(pred.numpy(), single_gt[i][0, :,:].numpy()))\n",
    "\n",
    "    # Store results in dictionaries\n",
    "    denoised_dict[key] = multi_predictions\n",
    "    uncertain_dict[key] = uncertainties\n",
    "    mse_dict[key] = mses\n",
    "    ssim_dict[key] = ssim_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8aff3-d506-4274-8fc7-500717f44a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at Uncertainty Trends for an MPM Sample\n",
    "sample = 28\n",
    "\n",
    "denoised_sample = denoised_dict[sample]\n",
    "uncertainty_sample = uncertain_dict[sample]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6)) \n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(denoised_sample[i * 5][200:500, 0:300], cmap = \"gray\")  \n",
    "    axes[0, i].axis(\"off\") \n",
    "    axes[0, i].set_title(f\"Iteration {i * 5}\")\n",
    "\n",
    "for i in range(4):\n",
    "    axes[1, i].imshow(uncertainty_sample[i * 5][200:500, 0:300], cmap=\"rainbow\", vmax = np.max(uncertainty_sample[15]))  \n",
    "    axes[1, i].axis(\"off\")  \n",
    "    axes[1, i].set_title(f\"Iteration {i * 5}\")\n",
    "\n",
    "# Set y-axis labels for rows\n",
    "fig.text(0.0, 0.75, \"Denoised\", va=\"center\", ha=\"center\", fontsize=14, rotation=90)\n",
    "fig.text(0.0, 0.25, \"Uncertainty\", va=\"center\", ha=\"center\", fontsize=14, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8dc449-6005-47ab-ab19-2a34c172ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percents = list(range(0, 2, 20))\n",
    "print(np.arange(0, 2, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd3d50-ac90-4e8b-bbca-7e39c479a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pix_scanned = torch.tensor(noisy_dict[28][0].shape).prod().item()\n",
    "\n",
    "# ADAPTIVE PART\n",
    "sample = 28\n",
    "noisy_ims_torch = noisy_dict[sample][-1]\n",
    "\n",
    "percents =  [0.01]\n",
    "\n",
    "adaptive_dict = {}\n",
    "uncertainty_dict = {}\n",
    "mse_error_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for percent in log_progress(percents):\n",
    "        adaptive_images = []\n",
    "        uncertainty_perc = []\n",
    "        num_pixels_scanned = []\n",
    "\n",
    "        # Prepare input image batch\n",
    "        noisy_0 = noisy_ims_torch[0]\n",
    "        in_img = noisy_0.repeat(20, 1, 1).unsqueeze(0).to(device)\n",
    "\n",
    "        # Initial model inference\n",
    "        out = finetuned(in_img.float())[0].detach().cpu()\n",
    "        uncertainty_0 = torch.clamp((out[2] - out[0]) * ch4_lambdas[-1], 0, 1)\n",
    "\n",
    "        num_pixels_scanned.append(in_img.shape[-1] ** 2)\n",
    "        unc_thresh = np.percentile(uncertainty_0, percent)\n",
    "        \n",
    "        for i in range(1, 20):\n",
    "            mask = uncertainty_0 >= unc_thresh\n",
    "            noisy_now = torch.where(mask, noisy_ims_torch[i], noisy_0)\n",
    "            num_pixels_scanned.append(mask.sum().item())\n",
    "\n",
    "            in_img[0, i] = noisy_now.to(device)\n",
    "\n",
    "            out = finetuned(in_img.float())[0].detach().cpu()\n",
    "            adaptive_images.append(out[1].clone())\n",
    "\n",
    "            uncertainty_0 = (out[2] - out[0]) * ch4_lambdas[-1]\n",
    "            uncertainty_perc.append(uncertainty_0.clone())\n",
    "\n",
    "        perc_pix_rescanned = sum(num_pixels_scanned) / tot_pix_scanned\n",
    "        print(\"percents = \", percent)\n",
    "        print(\"perc_pix_rescanned = \", perc_pix_rescanned)\n",
    "        adaptive_dict[percent] = adaptive_images\n",
    "        uncertainty_dict[percent] = uncertainty_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99522ea-b295-4a93-96d5-9aef6ef6886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_perc = adaptive_dict[0.01]\n",
    "uncertainty_perc = uncertainty_dict[0.01]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 6)) \n",
    "x = 0\n",
    "y = 250\n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(adaptive_perc[i * 5][375:425, 75:300], cmap = \"gray\")  \n",
    "    axes[0, i].axis(\"off\") \n",
    "    axes[0, i].set_title(f\"Iteration {i * 5}\")\n",
    "\n",
    "for i in range(4):\n",
    "    axes[1, i].imshow(uncertainty_perc[i * 5][375:425, 75:300], cmap=\"rainbow\", vmax = torch.max(uncertainty_perc[0]))  \n",
    "    axes[1, i].axis(\"off\")  \n",
    "    axes[1, i].set_title(f\"Iteration {i * 5}\")\n",
    "\n",
    "fig.text(0.0, 0.75, \"Adaptively Denoised (80%)\", va=\"center\", ha=\"center\", fontsize=14, rotation=90)\n",
    "fig.text(0.0, 0.25, \"Uncertainty\", va=\"center\", ha=\"center\", fontsize=14, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
